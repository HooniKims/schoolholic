# ğŸ”Œ ë¡œì»¬ LLM API ì—°ë™ ê°€ì´ë“œ (ì¬ì‚¬ìš© í…œí”Œë¦¿)

> ì´ ë¬¸ì„œëŠ” **Ollama ë¡œì»¬ LLM**ì„ ì›¹ í”„ë¡œì íŠ¸ì—ì„œ í˜¸ì¶œí•˜ëŠ” ì „ì²´ íŒ¨í„´ì„ ì •ë¦¬í•œ ê²ƒì…ë‹ˆë‹¤.  
> ë‹¤ë¥¸ í”„ë¡œì íŠ¸ì— ë³µì‚¬í•˜ì—¬ ë°”ë¡œ ì ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

---

## ğŸ“ ì•„í‚¤í…ì²˜ ê°œìš”

```
[ë¸Œë¼ìš°ì € (í•œêµ­)] â”€â”€ì§ì ‘ í˜¸ì¶œâ”€â”€â–¶ [api.alluser.site (Nginx í”„ë¡ì‹œ)] â”€â”€â–¶ [Ollama (192.168.0.182:11434)]
```

### ì™œ ë¸Œë¼ìš°ì € ì§ì ‘ í˜¸ì¶œì¸ê°€?

| ë°©ì‹ | ê²°ê³¼ | ì´ìœ  |
|------|------|------|
| Netlify ì„œë²„ë¦¬ìŠ¤ â†’ Ollama | âŒ ì‹¤íŒ¨ | í•´ì™¸ ì„œë²„ì—ì„œ í•œêµ­ ê°€ì • ë„¤íŠ¸ì›Œí¬ ì ‘ê·¼ ë¶ˆê°€ (ë°©í™”ë²½/ISP ì°¨ë‹¨) |
| Edge í•¨ìˆ˜ | âŒ ì‹¤íŒ¨ | OpenAI SDK í˜¸í™˜ ë¬¸ì œ (502) |
| ì„œë²„ë¦¬ìŠ¤ ìŠ¤íŠ¸ë¦¬ë° | âŒ ì‹¤íŒ¨ | Netlify 10ì´ˆ í•˜ë“œ íƒ€ì„ì•„ì›ƒ â†’ 504 |
| **ë¸Œë¼ìš°ì € â†’ Nginx í”„ë¡ì‹œ** | âœ… ì„±ê³µ | í•œêµ­ ë‚´ ì§ì ‘ í†µì‹ , íƒ€ì„ì•„ì›ƒ ì—†ìŒ |

---

## 1. Nginx í”„ë¡ì‹œ ì„¤ì • (`api.alluser.site`)

```nginx
location / {
    # ===== CORS ì„¤ì • =====
    set $cors_origin "";
    if ($http_origin = "https://your-app.netlify.app") { set $cors_origin $http_origin; }
    if ($http_origin = "http://localhost:3000") { set $cors_origin $http_origin; }
    # ğŸ”§ ìƒˆ í”„ë¡œì íŠ¸ ì¶”ê°€ ì‹œ ìœ„ì— í•œ ì¤„ë§Œ ì¶”ê°€

    if ($request_method = OPTIONS) {
        add_header 'Access-Control-Allow-Origin' $cors_origin always;
        add_header 'Access-Control-Allow-Methods' 'GET, POST, OPTIONS' always;
        add_header 'Access-Control-Allow-Headers' 'Content-Type, X-API-Key' always;
        add_header 'Access-Control-Max-Age' 86400 always;
        add_header 'Content-Length' 0;
        return 204;
    }

    add_header 'Access-Control-Allow-Origin' $cors_origin always;

    # ===== API Key ì¸ì¦ =====
    if ($http_x_api_key != "YOUR_API_KEY_HERE") {
        return 401 '{"error":"Unauthorized"}';
    }

    # ===== Proxy to Ollama =====
    proxy_pass http://192.168.0.182:11434;

    proxy_set_header Host $host;
    proxy_set_header X-Real-IP $remote_addr;
    proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
    proxy_set_header X-Forwarded-Proto $scheme;

    # Ollama ê¸°ë³¸ CORS í—¤ë” ì œê±° (ì¶©ëŒ ë°©ì§€)
    proxy_hide_header 'Access-Control-Allow-Origin';
    proxy_hide_header 'Access-Control-Allow-Methods';
    proxy_hide_header 'Access-Control-Allow-Headers';

    # Streaming / ì„±ëŠ¥ ìµœì í™”
    proxy_buffering off;
    proxy_request_buffering off;
    proxy_cache off;
    add_header X-Accel-Buffering no;

    proxy_http_version 1.1;
    proxy_set_header Connection "";

    # íƒ€ì„ì•„ì›ƒ (LLMì€ ì‘ë‹µì´ ëŠë¦´ ìˆ˜ ìˆìŒ)
    proxy_read_timeout 600s;
    proxy_connect_timeout 30s;
    proxy_send_timeout 600s;

    gzip off;
}
```

### ìƒˆ í”„ë¡œì íŠ¸ ì¶”ê°€ ì‹œ
```nginx
if ($http_origin = "https://ìƒˆí”„ë¡œì íŠ¸.netlify.app") { set $cors_origin $http_origin; }
```
ì´ í•œ ì¤„ë§Œ CORS ì„¤ì • ë¸”ë¡ì— ì¶”ê°€í•˜ë©´ ë©ë‹ˆë‹¤.

---

## 2. í´ë¼ì´ì–¸íŠ¸ ì½”ë“œ (ë³µì‚¬ìš© í…œí”Œë¦¿)

### 2-1. `utils/ollamaClient.js` â€” API í˜¸ì¶œ í•µì‹¬ ëª¨ë“ˆ

```javascript
// ===== ì„¤ì • =====
const OLLAMA_API_URL = "https://api.alluser.site";
const OLLAMA_API_KEY = "YOUR_API_KEY_HERE";

// ===== ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ ëª©ë¡ =====
export const AVAILABLE_MODELS = [
    { id: "qwen3:8b",              name: "Qwen 3 8B (ì¶”ì²œ)",    description: "ê· í˜• ì¡íŒ ì„±ëŠ¥" },
    { id: "gemma3:12b-it-q8_0",    name: "Gemma 3 12B Q8",      description: "ìµœê³  í’ˆì§ˆ (13GB)" },
    { id: "gemma3:12b-it-q4_K_M",  name: "Gemma 3 12B Q4",      description: "ê³ í’ˆì§ˆ (8GB)" },
    { id: "gemma3:4b-it-q4_K_M",   name: "Gemma 3 4B",          description: "ê²½ëŸ‰ (3.3GB)" },
    { id: "qwen3:4b",              name: "Qwen 3 4B",           description: "ê²½ëŸ‰ ë¹ ë¥¸ ì‘ë‹µ" },
    { id: "llama3.1:8b",           name: "Llama 3.1 8B",        description: "ë²”ìš© ëª¨ë¸" },
];

export const DEFAULT_MODEL = AVAILABLE_MODELS[0].id;

/**
 * Ollama API 1íšŒ í˜¸ì¶œ (OpenAI í˜¸í™˜ ì—”ë“œí¬ì¸íŠ¸)
 * 
 * @param {string} systemMessage - ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸
 * @param {string} userPrompt    - ì‚¬ìš©ì í”„ë¡¬í”„íŠ¸
 * @param {string} model         - ëª¨ë¸ ID (ê¸°ë³¸ê°’: DEFAULT_MODEL)
 * @param {Object} options       - ì¶”ê°€ ì˜µì…˜ { temperature, stream }
 * @returns {Promise<string>}    - ìƒì„±ëœ í…ìŠ¤íŠ¸
 */
export async function callOllamaAPI(systemMessage, userPrompt, model, options = {}) {
    const { temperature = 0.7, stream = false } = options;

    const res = await fetch(`${OLLAMA_API_URL}/v1/chat/completions`, {
        method: "POST",
        headers: {
            "Content-Type": "application/json",
            "X-API-Key": OLLAMA_API_KEY,
        },
        body: JSON.stringify({
            model: model || DEFAULT_MODEL,
            messages: [
                { role: "system", content: systemMessage },
                { role: "user", content: userPrompt },
            ],
            temperature,
            stream,
        }),
    });

    if (!res.ok) {
        let errorMessage = `ì„œë²„ ì˜¤ë¥˜ (${res.status})`;
        try {
            const errorData = await res.json();
            errorMessage = errorData.error || errorMessage;
        } catch {
            // ë¬´ì‹œ
        }
        throw new Error(errorMessage);
    }

    const data = await res.json();
    return data.choices?.[0]?.message?.content || "";
}

/**
 * ê³ ìˆ˜ì¤€ API: ì‹œìŠ¤í…œ ë©”ì‹œì§€ + í”„ë¡¬í”„íŠ¸ + ì¶”ê°€ ì§€ì¹¨ì„ ê²°í•©í•˜ì—¬ í˜¸ì¶œ
 * "Sandwich ê¸°ë²•" ì ìš© â€” ì¶”ê°€ ì§€ì¹¨ì„ ì‹œìŠ¤í…œ/ì‚¬ìš©ì í”„ë¡¬í”„íŠ¸ ì•ë’¤ì— ì‚½ì…
 * 
 * @param {Object} params
 * @param {string} params.systemMessage         - ê¸°ë³¸ ì‹œìŠ¤í…œ ë©”ì‹œì§€
 * @param {string} params.prompt                - ì‚¬ìš©ì í”„ë¡¬í”„íŠ¸
 * @param {string} [params.additionalInstructions] - ì¶”ê°€ ì§€ì¹¨ (ì„ íƒ)
 * @param {string} [params.model]               - ëª¨ë¸ ID (ì„ íƒ)
 * @returns {Promise<string>}
 */
export async function generateWithInstructions({ systemMessage, prompt, additionalInstructions, model }) {
    // ì¶”ê°€ ì§€ì¹¨ â†’ ì‹œìŠ¤í…œ ë©”ì‹œì§€ì— ì¶”ê°€
    let finalSystemMessage = systemMessage;
    if (additionalInstructions) {
        finalSystemMessage += `\n\nì‚¬ìš©ì ì¶”ê°€ ê·œì¹™ (ìµœìš°ì„  ì¤€ìˆ˜):\n${additionalInstructions}`;
    }

    // ì¶”ê°€ ì§€ì¹¨ â†’ ì‚¬ìš©ì í”„ë¡¬í”„íŠ¸ ì•ë’¤ì— ê°ì‹¸ê¸° (Sandwich ê¸°ë²•)
    let finalPrompt = prompt;
    if (additionalInstructions && additionalInstructions.trim()) {
        const prefix = `[ìµœìš°ì„  ê·œì¹™] ë‹¤ìŒ ê·œì¹™ì„ ë°˜ë“œì‹œ ì§€ì¼œì„œ ì‘ì„±í•˜ë¼: ${additionalInstructions}\n\n`;
        const suffix = `\n\n[ë‹¤ì‹œ í•œë²ˆ ê°•ì¡°] ìœ„ ë³¸ë¬¸ ì‘ì„± ì‹œ ë°˜ë“œì‹œ ì ìš©í•  ê·œì¹™: ${additionalInstructions}`;
        finalPrompt = prefix + prompt + suffix;
    }

    return callOllamaAPI(finalSystemMessage, finalPrompt, model);
}
```

### 2-2. ìë™ ì¬ì‹œë„ ë¡œì§ (ì™„ì „í•œ ë¬¸ì¥ ê²€ì¦)

```javascript
/**
 * í…ìŠ¤íŠ¸ê°€ ì™„ì „í•œ í•œêµ­ì–´ ë¬¸ì¥ìœ¼ë¡œ ëë‚˜ëŠ”ì§€ í™•ì¸
 */
function endsWithCompleteSentence(text) {
    if (!text || !text.trim()) return false;
    const trimmed = text.trim();
    return /[í•¨ìŒì„ë¨ë´„ì˜´ì¤Œì¶¤ì›€ëŠ ë¦„ë‹¤ìš”ê¹Œë‹ˆ][.!?]\s*$/.test(trimmed);
}

/**
 * ìë™ ì¬ì‹œë„ í¬í•¨ API í˜¸ì¶œ
 * ë¬¸ì¥ì´ ë¶ˆì™„ì „í•˜ê²Œ ëë‚˜ë©´ ìµœëŒ€ 2íšŒ ì¬ì‹œë„
 * 
 * @param {Object} params - generateWithInstructionsì™€ ë™ì¼í•œ íŒŒë¼ë¯¸í„°
 * @returns {Promise<string>}
 */
export async function generateWithRetry(params) {
    let content = await generateWithInstructions(params);

    if (!content.trim()) {
        throw new Error("AI ì‘ë‹µì´ ë¹„ì–´ìˆìŠµë‹ˆë‹¤.");
    }

    const MAX_RETRIES = 2;
    for (let retry = 0; retry < MAX_RETRIES; retry++) {
        if (endsWithCompleteSentence(content)) break;

        console.log(`[ì¬ì‹œë„ ${retry + 1}/${MAX_RETRIES}] ë¬¸ì¥ ë¶ˆì™„ì „: "...${content.slice(-30)}"`);

        const retryPrompt = `ë‹¤ìŒ í…ìŠ¤íŠ¸ëŠ” ë¬¸ì¥ì´ ì¤‘ê°„ì— ëŠê²¼ìŠµë‹ˆë‹¤. ê°™ì€ ë‚´ìš©ì„ ì™„ì „í•œ ë¬¸ì¥ìœ¼ë¡œ ëë‚˜ë„ë¡ ë‹¤ì‹œ ì‘ì„±í•˜ì„¸ìš”. ë°˜ë“œì‹œ ì¢…ê²°ì–´ë¯¸ì™€ ë§ˆì¹¨í‘œë¡œ ëë‚´ì„¸ìš”. ì˜¤ì§ ë³¸ë¬¸ë§Œ ì¶œë ¥í•˜ì„¸ìš”.\n\në¶ˆì™„ì „í•œ í…ìŠ¤íŠ¸:\n${content}`;

        const retryContent = await callOllamaAPI(params.systemMessage, retryPrompt, params.model);

        if (retryContent.trim() && endsWithCompleteSentence(retryContent)) {
            content = retryContent;
            console.log(`[ì¬ì‹œë„ ì„±ê³µ] ì™„ì „í•œ ë¬¸ì¥ìœ¼ë¡œ ìˆ˜ì •ë¨`);
            break;
        } else if (retryContent.trim()) {
            content = retryContent;
        }
    }

    return content;
}
```

---

## 3. í…ìŠ¤íŠ¸ í›„ì²˜ë¦¬ ìœ í‹¸ (`utils/textProcessor.js`)

AI ìƒì„± í…ìŠ¤íŠ¸ì˜ ê¸€ììˆ˜ ì œí•œ ë° ì •ë¦¬ë¥¼ ìœ„í•œ ìœ í‹¸ë¦¬í‹°ì…ë‹ˆë‹¤.

### 3-1. ë©”íƒ€ ì •ë³´ ì œê±°

```javascript
/**
 * AI ì¶œë ¥ì—ì„œ ë©”íƒ€ ì •ë³´(ê¸€ììˆ˜, ë¶„ì„ ë‚´ìš© ë“±) ì œê±°
 */
export function cleanMetaInfo(text) {
    if (!text) return text;

    // ê´„í˜¸ ì•ˆì˜ ë©”íƒ€ ì •ë³´: (ì•½ 500ì), (ê¸€ììˆ˜: 330) ë“±
    let cleaned = text.replace(/\s*\([^)]*\d+ì[^)]*\)/g, '');
    cleaned = cleaned.replace(/\s*\([^)]*ê¸€ì[^)]*\)/g, '');
    cleaned = cleaned.replace(/\s*\([^)]*ìì„¸í•œ[^)]*\)/g, '');
    cleaned = cleaned.replace(/\s*\([^)]*ë‚´ìš©\s*í¬í•¨[^)]*\)/g, '');

    // ëë¶€ë¶„: "--- 330ì" ë˜ëŠ” "[330ì]"
    cleaned = cleaned.replace(/\s*[-â”€]+\s*\d+ì\s*$/g, '');
    cleaned = cleaned.replace(/\s*\[\d+ì\]\s*$/g, '');
    cleaned = cleaned.replace(/\s*\d+ì\s*$/g, '');

    // ë¶„ì„/ê²€ì¦ ê´€ë ¨ ë¬¸êµ¬ ì œê±°
    cleaned = cleaned.replace(/\s*\[ë¶„ì„[^\]]*\]/g, '');
    cleaned = cleaned.replace(/\s*\[ê²€ì¦[^\]]*\]/g, '');

    return cleaned.trim();
}
```

### 3-2. ê¸€ììˆ˜ ì´ˆê³¼ ì‹œ ì™„ì „í•œ ë¬¸ì¥ìœ¼ë¡œ ìë¥´ê¸°

```javascript
const MAX_CHARS = 500; // ì ˆëŒ€ ìƒí•œì„ 

function isCompleteSentence(text) {
    if (!text) return false;
    return /[í•¨ìŒì„ë¨ë´„ì˜´ì¤Œì¶¤ì›€ëŠ ë¦„ë‹¤ìš”ê¹Œë‹ˆ][.!?]\s*$/.test(text.trim());
}

function splitIntoSentences(text) {
    if (!text) return [];
    return text.split(/(?<=[.!?])\s+/).filter(s => s.trim());
}

/**
 * ê¸€ììˆ˜ ì´ˆê³¼ ì‹œ ë§ˆì§€ë§‰ ì™„ì „í•œ ë¬¸ì¥ê¹Œì§€ë§Œ ìœ ì§€
 * 
 * @param {string} text        - AI ìƒì„± í…ìŠ¤íŠ¸
 * @param {number} targetChars - ëª©í‘œ ê¸€ììˆ˜
 * @returns {string}
 */
export function truncateToCompleteSentence(text, targetChars) {
    let cleaned = cleanMetaInfo(text);
    if (!cleaned) return '';

    const maxAllowed = Math.min(targetChars, MAX_CHARS);

    // ì´ë¯¸ ì œí•œ ë‚´ì´ê³  ì™„ì „í•œ ë¬¸ì¥ì´ë©´ ê·¸ëŒ€ë¡œ ë°˜í™˜
    if (cleaned.length <= maxAllowed && isCompleteSentence(cleaned)) {
        return cleaned.trim();
    }

    // ë¬¸ì¥ ë‹¨ìœ„ë¡œ ë¶„ë¦¬ â†’ ê¸€ììˆ˜ ë‚´ì—ì„œ ìµœëŒ€í•œ ë§ì€ ë¬¸ì¥ í¬í•¨
    const sentences = splitIntoSentences(cleaned);
    let result = '';

    for (const sentence of sentences) {
        const trimmed = sentence.trim();
        const complete = /[.!?]$/.test(trimmed) ? trimmed : trimmed + '.';
        const candidate = result + (result ? ' ' : '') + complete;

        if (candidate.length <= maxAllowed) {
            result = candidate;
        } else {
            break;
        }
    }

    return result.trim();
}
```

### 3-3. ê¸€ììˆ˜ ì§€ì¹¨ í”„ë¡¬í”„íŠ¸ ìƒì„±ê¸°

```javascript
/**
 * AIì—ê²Œ ë³´ë‚¼ ê¸€ììˆ˜ ê´€ë ¨ í”„ë¡¬í”„íŠ¸ ì§€ì¹¨ ìƒì„±
 * 
 * @param {number} targetChars - ëª©í‘œ ê¸€ììˆ˜
 * @returns {string} - í”„ë¡¬í”„íŠ¸ì— ì‚½ì…í•  ì§€ì¹¨ ë¬¸ìì—´
 */
export function getCharacterGuideline(targetChars) {
    const maxAllowed = Math.min(targetChars, MAX_CHARS);

    // ì§§ì€ ê¸€ì¼ìˆ˜ë¡ ë²„í¼ë¥¼ ë” í¬ê²Œ
    let bufferRatio;
    if (targetChars <= 100) bufferRatio = 0.70;
    else if (targetChars <= 150) bufferRatio = 0.75;
    else if (targetChars <= 200) bufferRatio = 0.80;
    else if (targetChars <= 300) bufferRatio = 0.85;
    else bufferRatio = 0.90;

    const promptLimit = Math.floor(maxAllowed * bufferRatio);

    return `
<ê¸€ììˆ˜ ì œí•œ>
ì „ì²´ ê¸€ììˆ˜: ${maxAllowed}ì ì´í•˜ (ê³µë°± í¬í•¨, ì´ˆê³¼ ë¶ˆê°€)
ëª©í‘œ: ${promptLimit}ì ~ ${maxAllowed}ì

ì‘ì„± ë°©ë²•:
1. ${maxAllowed}ì ì œí•œì„ ì¸ì§€í•˜ê³  ê³„íšì ìœ¼ë¡œ ì‘ì„±
2. ëª¨ë“  ë¬¸ì¥ì€ ì™„ì „í•œ ì¢…ê²°ì–´ë¯¸ë¡œ ëëƒ„
3. ìµœì¢… ì¶œë ¥ì€ ${maxAllowed}ì ì´í•˜, ì™„ì „í•œ ë¬¸ì¥ìœ¼ë¡œ ëëƒ„
`;
}
```

---

## 4. í˜ì´ì§€ì—ì„œì˜ ì‚¬ìš© ì˜ˆì‹œ

### React (Next.js) ê¸°ì¤€

```javascript
"use client";

import { useState } from "react";
import { generateWithRetry, AVAILABLE_MODELS, DEFAULT_MODEL } from "../../utils/ollamaClient";
import { truncateToCompleteSentence, cleanMetaInfo, getCharacterGuideline } from "../../utils/textProcessor";

export default function MyPage() {
    const [result, setResult] = useState("");
    const [isLoading, setIsLoading] = useState(false);
    const [selectedModel, setSelectedModel] = useState(DEFAULT_MODEL);

    const handleGenerate = async () => {
        setIsLoading(true);
        try {
            const systemMessage = "ë‹¹ì‹ ì€ ì „ë¬¸ ì‘ì„± ë„ìš°ë¯¸ì…ë‹ˆë‹¤.";
            const prompt = `ë‹¤ìŒ ì£¼ì œë¡œ ê¸€ì„ ì‘ì„±í•˜ì„¸ìš”: ...`;
            const additionalInstructions = ""; // ì‚¬ìš©ì ì¶”ê°€ ì§€ì¹¨ (ì„ íƒ)

            // 1. AI ìƒì„±
            const rawResult = await generateWithRetry({
                systemMessage,
                prompt,
                additionalInstructions,
                model: selectedModel,
            });

            // 2. í›„ì²˜ë¦¬ (ê¸€ììˆ˜ ì œí•œ ì ìš©)
            const processed = truncateToCompleteSentence(rawResult, 500);

            setResult(processed);
        } catch (error) {
            console.error(error);
            alert(`ìƒì„± ì‹¤íŒ¨: ${error.message}`);
        } finally {
            setIsLoading(false);
        }
    };

    return (
        <div>
            {/* ëª¨ë¸ ì„ íƒ UI */}
            <select value={selectedModel} onChange={e => setSelectedModel(e.target.value)}>
                {AVAILABLE_MODELS.map(m => (
                    <option key={m.id} value={m.id}>{m.name}</option>
                ))}
            </select>

            <button onClick={handleGenerate} disabled={isLoading}>
                {isLoading ? "ìƒì„± ì¤‘..." : "AI ìƒì„±"}
            </button>

            <textarea value={result} readOnly />
        </div>
    );
}
```

---

## 5. API í˜¸ì¶œ í•µì‹¬ ì •ë¦¬

### ì—”ë“œí¬ì¸íŠ¸

| í•­ëª© | ê°’ |
|------|-----|
| URL | `https://api.alluser.site/v1/chat/completions` |
| Method | `POST` |
| ì¸ì¦ | `X-API-Key` í—¤ë” |
| Content-Type | `application/json` |

### Request Body

```json
{
    "model": "qwen3:8b",
    "messages": [
        { "role": "system", "content": "ì‹œìŠ¤í…œ ë©”ì‹œì§€" },
        { "role": "user", "content": "ì‚¬ìš©ì í”„ë¡¬í”„íŠ¸" }
    ],
    "temperature": 0.7,
    "stream": false
}
```

### Response (OpenAI í˜¸í™˜ í˜•ì‹)

```json
{
    "choices": [
        {
            "message": {
                "role": "assistant",
                "content": "ìƒì„±ëœ í…ìŠ¤íŠ¸"
            }
        }
    ]
}
```

### ì‘ë‹µ í…ìŠ¤íŠ¸ ì¶”ì¶œ

```javascript
const data = await res.json();
const content = data.choices?.[0]?.message?.content || "";
```

---

## 6. Ollama ì„œë²„ ê´€ë¦¬ (Mac mini)

### ëª¨ë¸ ì˜ˆì—´ (cold start ë°©ì§€)
```bash
# ëª¨ë¸ì„ 24ì‹œê°„ ë™ì•ˆ ë©”ëª¨ë¦¬ì— ìœ ì§€
ollama run qwen3:8b "ì•ˆë…•" --keepalive 24h
```

### ìƒíƒœ í™•ì¸
```bash
ollama ps        # í˜„ì¬ ë¡œë“œëœ ëª¨ë¸
ollama list      # ì„¤ì¹˜ëœ ëª¨ë¸ ëª©ë¡
```

### ëª¨ë¸ ìƒì‹œ ìœ ì§€ (cron ìë™í™”)
```bash
# crontab -e â†’ 4ë¶„ë§ˆë‹¤ pingìœ¼ë¡œ ëª¨ë¸ ìœ ì§€
*/4 * * * * curl -s http://localhost:11434/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{"model":"qwen3:8b","messages":[{"role":"user","content":"ping"}]}' \
  > /dev/null 2>&1
```

---

## 7. ë””ë²„ê¹… ì²´í¬ë¦¬ìŠ¤íŠ¸

ë¬¸ì œ ë°œìƒ ì‹œ ì•„ë˜ ìˆœì„œëŒ€ë¡œ í™•ì¸:

| # | í™•ì¸ ì‚¬í•­ | í™•ì¸ ë°©ë²• |
|---|-----------|-----------|
| 1 | Ollama ì‹¤í–‰ ì¤‘? | `ollama ps` |
| 2 | ëª¨ë¸ ë¡œë“œë¨? | `ollama ps` â†’ ëª¨ë¸ëª… í‘œì‹œ í™•ì¸ |
| 3 | ë¡œì»¬ API ì‘ë‹µ? | `curl http://localhost:11434/v1/chat/completions -H "Content-Type: application/json" -d '{"model":"qwen3:8b","messages":[{"role":"user","content":"hi"}]}'` |
| 4 | í”„ë¡ì‹œ ê²½ìœ  ì‘ë‹µ? | `curl https://api.alluser.site/v1/chat/completions -H "Content-Type: application/json" -H "X-API-Key: YOUR_KEY" -d '{"model":"qwen3:8b","messages":[{"role":"user","content":"hi"}]}'` |
| 5 | Nginx ìƒíƒœ? | `sudo nginx -t` |
| 6 | CORS ì—ëŸ¬? | ë¸Œë¼ìš°ì € ì½˜ì†” â†’ `Access-Control-Allow-Origin` ì—ëŸ¬ í™•ì¸ |
| 7 | ëª¨ë¸ cold start? | `ollama run qwen3:8b "test" --keepalive 24h` |

---

## 8. ìƒˆ í”„ë¡œì íŠ¸ ì ìš© ì²´í¬ë¦¬ìŠ¤íŠ¸

1. â˜ `utils/ollamaClient.js` ë³µì‚¬ â†’ API_URL, API_KEY ì„¤ì •
2. â˜ `utils/textProcessor.js` ë³µì‚¬ (ê¸€ììˆ˜ ì œí•œì´ í•„ìš”í•œ ê²½ìš°)
3. â˜ Nginx CORSì— ìƒˆ ë„ë©”ì¸ ì¶”ê°€
4. â˜ ì‚¬ìš©í•  ëª¨ë¸ í™•ì¸ (`ollama list`)
5. â˜ ëª¨ë¸ ì˜ˆì—´ (`ollama run MODEL "test" --keepalive 24h`)
6. â˜ í˜ì´ì§€ì—ì„œ import í›„ í˜¸ì¶œ í…ŒìŠ¤íŠ¸

---

## 9. í”„ë¡¬í”„íŠ¸ ìµœì í™” íŒ (ë¡œì»¬ LLM ì „ìš©)

### Sandwich ê¸°ë²•
ì¶”ê°€ ì§€ì¹¨ì„ LLMì´ ì˜ ë”°ë¥´ë„ë¡ **ì‹œìŠ¤í…œ ë©”ì‹œì§€ + í”„ë¡¬í”„íŠ¸ ì• + í”„ë¡¬í”„íŠ¸ ë’¤** 3ê³³ì— ì‚½ì…:

```javascript
// ì‹œìŠ¤í…œ ë©”ì‹œì§€ì— ì¶”ê°€
systemMessage += `\n\nì‚¬ìš©ì ì¶”ê°€ ê·œì¹™:\n${instructions}`;

// í”„ë¡¬í”„íŠ¸ ì•ë’¤ì— ê°ì‹¸ê¸°
finalPrompt = `[ìµœìš°ì„  ê·œì¹™] ${instructions}\n\n` + prompt + `\n\n[ë‹¤ì‹œ ê°•ì¡°] ${instructions}`;
```

### ë¡œì»¬ LLM í”„ë¡¬í”„íŠ¸ ì£¼ì˜ì‚¬í•­
- **ê¸ì •í˜• ì§€ì‹œ**: "~í•˜ì§€ ë§ˆì„¸ìš”" ëŒ€ì‹  "~ë§Œ ì‚¬ìš©í•˜ì„¸ìš”"
- **ê°„ê²°í•œ ê·œì¹™**: ë§ì€ ê·œì¹™ë³´ë‹¤ í•µì‹¬ 5~7ê°œë§Œ
- **ì˜ˆì‹œ ì œê³µ**: ì¢‹ì€ ì¶œë ¥ ì˜ˆì‹œë¥¼ í¬í•¨í•˜ë©´ í’ˆì§ˆ í–¥ìƒ
- **ë©”íƒ€ ì •ë³´ ì œê±°**: AIê°€ "(ì•½ 300ì)" ê°™ì€ ë©”íƒ€ ì •ë³´ë¥¼ ì¶œë ¥í•  ìˆ˜ ìˆìœ¼ë¯€ë¡œ í›„ì²˜ë¦¬ í•„ìˆ˜
